{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4799af4e-ac5e-4be1-b759-246652c33b45",
   "metadata": {},
   "source": [
    "## Machine Learning Assignment 4 Coding Part\n",
    "\n",
    "This Jupyter Notebook contains the coding part of Assignment 4. Please enter your name and student ID here:\n",
    "\n",
    "- Name: \n",
    "- StudentID:\n",
    "\n",
    "### Overview\n",
    "\n",
    "In the coding part of Assignment 4, you'll complete the following four tasks:\n",
    "\n",
    "- **Task 1**: Implement $k$​​​-means.\n",
    "- **Task 2**: Implement $k$-means++ initialization.\n",
    "- **Task 3**: Implement a Gaussian Mixture Model.\n",
    "\n",
    "The following code is incomplete, and you are required to fill in the missing parts. Please feel free to define your own functions or modify the functions given if needed. You may not use off-the-shelf libraries for all three tasks.\n",
    "\n",
    "### Data Format\n",
    "\n",
    "We will be working with a toy dataset with features in $\\mathbb R^2$. Before you begin, download the following files.\n",
    "- `train.txt` the training set, containing 4800 samples.\n",
    "- `dev.txt` the development set, containing 800 samples.\n",
    "\n",
    "Each line of the files stores a sample in the format `feature_1,feature_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0a091-0213-467b-8878-1f3b00090727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import Tuple, Union, Type, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148cf69-76e2-48da-9f0b-0f01b6415561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str, types: Tuple[Type, ...]) -> Tuple[ndarray, ...]:\n",
    "    \"\"\"Read a text file with lines containing comma separated numerical values.\n",
    "    Args:\n",
    "        file_path (str): The file path.\n",
    "        types (Tuple[Type]): The types of each column, for example, (float, float, int).\n",
    "    Returns:\n",
    "        columns (Tuple[ndarray, ...]): Numpy arrays of shape [N_lines].\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        items = [line.strip().split(\",\") for line in f.readlines()]\n",
    "        items = [tuple(type(e) for type, e in zip(types, t)) for t in items]\n",
    "    columns = list(zip(*items))\n",
    "    return tuple(np.asarray(c) for c in columns)\n",
    "\n",
    "# Load the dataset from files\n",
    "train_x1, train_x2 = read_file(\"train.txt\", (float, float))\n",
    "train_x = np.stack((train_x1, train_x2), axis=1)\n",
    "dev_x1, dev_x2 = read_file(\"dev.txt\", (float, float))\n",
    "dev_x = np.stack((dev_x1, dev_x2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16479cb0-5d39-4704-9402-998f3116e007",
   "metadata": {},
   "source": [
    "### Visualization of the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537c955-3d37-423f-b810-6f5c79ec9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=\"red\", s=0.5)\n",
    "plt.title(\"Scatter Plot of Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8015d2-4b8a-4688-b1c8-18b83b423059",
   "metadata": {},
   "source": [
    "### Task 1: Implement $k$-means\n",
    "\n",
    "For $\\mathbf x \\in \\mathbb R^D$ and finite set $\\mathcal C \\subset \\mathbb R^D$ define $d(\\mathbf x, \\mathcal C) := \\min_{\\mathbf c \\in \\mathcal C} \\|\\mathbf x - \\mathbf c\\|_2$.\n",
    "\n",
    "For finite set $\\mathcal X \\subset \\mathbb R^D$ and finite set $\\mathcal C \\subset \\mathbb R^D$ define $\\phi(\\mathcal X, \\mathcal C) := \\sum_{\\mathbf x \\in \\mathcal X} d^2(\\mathbf x, \\mathcal C)$.\n",
    "\n",
    "Given $\\mathcal X = \\{\\mathbf x_1, \\cdots, \\mathbf x_N\\} \\subset \\mathbb R^D$, $k$-means optimizes $\\mathcal C \\subset \\mathbb R^D$, where $|\\mathcal C| = K$, to minimize the following loss:\n",
    "$$\n",
    "\\mathcal L(\\mathcal C) = \\phi(\\mathcal X, \\mathcal C) = \\sum_{\\mathbf x \\in \\mathcal X} \\min_{\\mathbf c \\in \\mathcal C} \\|\\mathbf x - \\mathbf c\\|_2^2\n",
    "$$\n",
    "\n",
    "#### Subtasks\n",
    "1. Implement the `solve_k_means()` function in the following code block.\n",
    "2. Plot the loss $\\mathcal L$ as a function the number of updates to $\\mathcal C$.\n",
    "3. Generate a scatter plot of the training samples and cluster centers, after $k$-means converges to a **global minimum**.\n",
    "4. Run $k$-means multiple times. How frequently does the algorithm converge to a global minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800d7c8-9624-4b0f-af21-eaa495b9f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_k_means(\n",
    "    x: ndarray, c: ndarray, *, max_step: int=10000\n",
    ") -> Tuple[ndarray, ndarray]:\n",
    "    \"\"\"K-means solver. Given K initial cluster centers, update c iteratively.\n",
    "    Note:\n",
    "        - You may change the function signature.\n",
    "        - Please try your best to write vectorized code (i.e. avoid for loops over indices).\n",
    "        - Design some criterion for stopping the iteration.\n",
    "    Args:\n",
    "        x (ndarray[float]): shape [N, D], storing N data samples. D is the feature dimension.\n",
    "        c (ndarray[float]): shape [K, D], storing K initial cluster centers.\n",
    "        max_step (int): Maximum number of steps in $k$-means iteration.\n",
    "    Returns:\n",
    "        c (ndarray[float]): shape [K, D], updated K cluster centers after iterations.\n",
    "        index (ndarray[int]): shape [N], the index of the nearest cluster center of\n",
    "            each sample, in range {0, ..., K - 1}.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "    ##############################################################\n",
    "    # TODO: Enter your code here                                 #\n",
    "    ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0506f-ab14-4516-b1b2-3019641e3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random (standard Normal) initialization:\n",
    "def standard_normal_initialization(K: int) -> ndarray:\n",
    "    \"\"\"Standard normal initialization of K cluster centers.\n",
    "    Args:\n",
    "        K (int): number of cluster centers.\n",
    "    Returns:\n",
    "        c (ndarray): shape [K, 2], K initial cluster centers.\n",
    "    \"\"\"\n",
    "    return np.random.randn(K, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583cc59d-4f71-4c98-a926-2b8c61cfc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-means with random initialization:\n",
    "init_c = standard_normal_initialization(8)\n",
    "updated_c, index = solve_k_means(train_x, init_c, max_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708af48-77fb-4b49-b5fc-90db7cd93ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the training samples and the cluster centers.\n",
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=index, s=0.5, label=\"Training Samples\")\n",
    "plt.scatter(updated_c[:, 0], updated_c[:, 1], c=\"red\", s=50, label=\"Cluster Center\", marker=\"o\")\n",
    "\n",
    "plt.legend(title=\"Legend\")\n",
    "plt.title(\"Training Samples and $k$-means Cluster Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a2647-c9f0-47c0-a37e-7b4422912017",
   "metadata": {},
   "source": [
    "### Task 2: Implement $k$-means++\n",
    "\n",
    "$k$-means is sensitive to its initialization, and a random initialization of $\\mathbf c$ often results in convergence to a local minimum. $k$-means++ is an improved initialization method that often leads to better convergence. Read more about it in [Scalable K-Means++](https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).\n",
    "\n",
    "**(Algorithm)** Given $\\mathcal X$, $k$-means++ generates a set of $k$ vectors $\\mathcal I$ in $\\mathbb R^D$ using the following steps:\n",
    "1. First, randomly pick a sample $\\mathbf x$ from $\\mathcal X$. And set $\\mathcal I := \\{\\mathbf x\\}$.\n",
    "2. While $|\\mathcal I| < k$. Sample $\\mathbf x \\in \\mathcal X$ with probability $p(\\mathbf x) := d^2(\\mathbf x, \\mathcal I) / \\phi(\\mathcal X, \\mathcal I)$. And add the sampled $\\mathbf x$ to set $\\mathcal I$.\n",
    "\n",
    "#### Subtasks\n",
    "1. Implement $k$-means++ initialization algorithm.\n",
    "2. Compare the loss $\\mathcal L$ after convergence with standard normal initialization and $k$-means++ initialization.\n",
    "3. Compare the convergence speed with random and $k$-means++ initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878d190-e895-400b-875d-3c09e60bb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_pp_initialization(x: ndarray, K: int) -> ndarray:\n",
    "    \"\"\"K-means++ initialization method.\n",
    "    Args:\n",
    "        x (ndarray): shape [N, D], storing N data samples. D is the feature dimension.\n",
    "        K (int): Number of cluster centers.\n",
    "    Returns:\n",
    "        c (ndarray): shape [K, D], K initial cluster centers generated by K-means++.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "    ##############################################################\n",
    "    # TODO: Enter your code here                                 #\n",
    "    ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de410ee8-985a-419b-89c1-f790c2df1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cluster centers with k-means++, then do k-means updates.\n",
    "init_c = k_means_pp_initialization(train_x, 8)\n",
    "updated_c, index = solve_k_means(train_x, init_c, max_step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323dad7a-e680-42d1-9daf-ce9a7b49e9a1",
   "metadata": {},
   "source": [
    "### Task 3: Implement a Gaussian Mixture Model\n",
    "\n",
    "In this task, you will implement the Expectation-Maximization (EM) algorithm for learning a Gaussian Mixture Model (GMM). Our GMM in $\\mathbb R^2$ has a density given by:\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{k = 1}^K \\pi_k \\mathcal N(x| \\mu_k, \\Sigma_k), \\quad \\mu_k \\in \\mathbb R^2, \\quad \\Sigma_k \\in \\mathbb R^{2 \\times 2}\n",
    "$$\n",
    "\n",
    "#### Subtasks\n",
    "1. Implement EM algorithm. Hint: initialize the Gaussian means with $k$-means++ for better convergence.\n",
    "2. Use the provided `plot_gmm_contour` function to generate a 2D contour plot of the GMM density $p(x)$ after the log likelihood convergence to a global maximum. You may use `gmm_log_prob` to compute log probability of the GMM.\n",
    "3. Report the log-likelihood on the training set and development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc8c45-2187-44fa-b44f-25b535c15423",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError()\n",
    "##############################################################\n",
    "# TODO: Enter your code here                                 #\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a61f6-9e17-4495-8ae8-9c5f700f5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_log_prob(x: ndarray, pi: ndarray, mu: ndarray, sigma: ndarray) -> ndarray:\n",
    "    \"\"\"Batched computation of GMM log probability.\n",
    "    Arg:\n",
    "        x (ndarray): [..., 2]. Each sample is a vector in R^2.\n",
    "        pi (ndarray): [K], mixture weights in the GMM, pi.sum() = 1.\n",
    "        mu (ndarray): [K, 2], means of Gaussian components.\n",
    "        sigma (ndarray): [K, 2, 2], covariance matrix of Gaussian components.\n",
    "            Each sigma[k] must be symmetric positive definite.\n",
    "    Returns:\n",
    "        log_prob (ndarray): [...]. The log probability of each vector in x.\n",
    "            Has the same shape as x after removing the last dimension of x.\n",
    "    \"\"\"\n",
    "    mix = torch.distributions.Categorical(probs=torch.as_tensor(pi))\n",
    "    normals = torch.distributions.MultivariateNormal(\n",
    "        loc=torch.as_tensor(mu), covariance_matrix=torch.as_tensor(sigma)\n",
    "    )\n",
    "    gmm = torch.distributions.MixtureSameFamily(mix, normals)\n",
    "    return gmm.log_prob(torch.as_tensor(x)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ebb70a-63cb-452b-8e32-c6ccc17cc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm_contour(x: ndarray, pi: ndarray, mu: ndarray, sigma: ndarray) -> None:\n",
    "    \"\"\"Generate a contour plot of the GMM probability, overlapped with a scatter plot\n",
    "        of the samples in x.\n",
    "    Args:\n",
    "        x (ndarray): [N, 2].\n",
    "        pi (ndarray): [K], mixture weights in the GMM, pi.sum() = 1.\n",
    "        mu (ndarray): [K, 2], means of Gaussian components.\n",
    "        sigma (ndarray): [K, 2, 2], covariance matrix of Gaussian components.\n",
    "            Each sigma[k] must be symmetric positive definite.\n",
    "    \"\"\"\n",
    "    grid_1 = np.arange(-1.5, 4, 0.01)  # [Gx]\n",
    "    grid_2 = np.arange(-2, 3, 0.01)  # [Gy]\n",
    "    grid_11, grid_22 = np.meshgrid(grid_1, grid_2, indexing=\"ij\")  # [Gx, Gy]\n",
    "    grid_12 = np.stack([grid_11, grid_22], axis=-1)  # [Gx, Gy, 2]\n",
    "    \n",
    "    grid_logp = gmm_log_prob(grid_12, pi, mu, sigma)  # [Gx, Gy, 2]\n",
    "\n",
    "    # Plot the contour\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.contour(grid_1, grid_2, np.exp(grid_logp.transpose()), levels=10, cmap='viridis')\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=\"red\", s=0.1, label=\"Training Samples\")\n",
    "    plt.scatter(mu[:, 0], mu[:, 1], c=\"black\", s=50, label=\"Gaussian Means\")\n",
    "    plt.legend()  # Add legend\n",
    "    plt.title(\"Contour Plot of GMM Probability with Training Samples\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
